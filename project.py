from dotenv import load_dotenv
import streamlit as st
import os
from PyPDF2 import PdfReader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import re
from langdetect import detect
from pyarabic.araby import normalize_hamza, strip_tatweel, strip_tashkeel 

def preparethePDF():
  text = ""
  pdf_reader = PdfReader("HR.pdf")
  for page in pdf_reader.pages:
    text+=page.extract_text()
   
  text_splitter = CharacterTextSplitter(
      separator="\n",
      chunk_size=3000,
      chunk_overlap=400,
      length_function=len
    )
  chunks = text_splitter.split_text(text=text)

  preprocessed_chunks = [
    chunk if detect(chunk) != 'ar' else re.sub(r'[,\t\n\r\x0b\x0c]', ' ', strip_tatweel(strip_tashkeel(chunk))).strip()
    for chunk in chunks
  ]
  embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large",
                    model_kwargs={'device': 'cpu'})
  
  vectorstore = FAISS.from_texts(
        texts=preprocessed_chunks,
        embedding=embeddings)  
   
  return vectorstore  

def process_question(user_question, vectorstore):
  with st.spinner('Please wait for responseğŸ˜Š...'):     
    llm = ChatGoogleGenerativeAI(model='gemini-pro',
                   google_api_key=os.getenv("GOOGLE_API_KEY"),
                  temperature=0.5,
                  convert_system_message_to_human=True)
    prompt_template = """

            **Ø§Ù†Øª Ø®Ø¨ÙŠØ± Ø§Ø³ØªØ´Ø§Ø±ÙŠ ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¨Ø´Ø±ÙŠØ©**
            **Ø§Ù„Ø³ÙŠØ§Ù‚: **
            {context}
            **Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª: **
            Ø³ÙˆÙ ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø¥Ø³ØªØ´Ø§Ø±ØªÙƒ Ø£Ùˆ Ø³Ø¤Ø§Ù„Ùƒ Ø¹Ù† Ø´ÙŠØ¡Ù‹ Ù…ØªØ¹Ù„Ù‚ ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¨Ø´Ø±ÙŠØ© ÙˆØ§Ù†Øª Ø¨Ø¯ÙˆØ±Ùƒ ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙ‡ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø¥Ø¹Ø·Ø§Ø¡ Ø¬ÙˆØ§Ø¨ ØµØ­ÙŠØ­ ÙˆÙ…ÙˆØ¬Ø² .
            '.Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† ØªØ¹Ø±Ù Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙ‚Ù„ Ù„Ù‡ 'Ø¥Ù†Ùƒ Ù„Ø§ ØªØ¹Ù„Ù… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ø­Ø§ÙˆÙ„ Ø£Ù† ØªØµÙŠØº Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø·Ø±ÙŠÙ‚Ø©Ù Ø£Ø®Ø±Ù‰.

            Ø¥Ø°Ø§ Ù‚Ø§Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø³Ø¤Ø§Ù„Ùƒ Ø¹Ù† Ø´ÙŠØ¡ ØºÙŠØ± Ù…ØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¨Ø´Ø±ÙŠØ© ÙÙ‚Ù„ Ù„Ù‡ Ø¥Ù†Ùƒ
            'ÙŠØ¬Ø¨ Ø£Ù† ØªÙ‚ÙˆÙ… Ø¨Ø³Ø¤Ø§Ù„ÙŠ Ø¹Ù† Ø´ÙŠØ¡ Ù…ØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¨Ø´Ø±ÙŠØ©'.
            {question}
            **Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: **

          """.strip()
    prompt = ChatPromptTemplate.from_template(prompt_template)
    retriever = vectorstore.as_retriever()
    chain = (
          {"context": retriever, "question": RunnablePassthrough()}
          | prompt
          | llm
          | StrOutputParser()
        )
    response = chain.invoke(user_question)

    return response
   
def main():
  load_dotenv()
  st.set_page_config(page_title="Ask your AI assistant-HR consultant  ğŸ¦œï¸ğŸ”—")
  st.markdown("<h1 class='title'>AI assistant-HR consultant</h1>", unsafe_allow_html=True)
  
  user_question = st.text_input("Ask your question")
  
  embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large",
                    model_kwargs={'device': 'cpu'})
  if user_question:
    # vectorstore = preparethePDF()
    # Save the vector store to a file
     
    # vectorstore.save_local("vectorstore_V2.faiss")
    # Load the vector store from a file

    vectorstore = FAISS.load_local("vectorstore_V2.faiss",embeddings)
    response = process_question(user_question, vectorstore)
    st.write(response)


if __name__ == '__main__':
  main() 