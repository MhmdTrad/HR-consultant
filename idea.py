# import streamlit as st
# from streamlit_chat import message
# import tempfile
# from langchain.document_loaders.csv_loader import CSVLoader
# from langchain_community.embeddings import HuggingFaceEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain_community.llms import CTransformers
# from langchain.chains import ConversationalRetrievalChain
# from langchain_google_genai import ChatGoogleGenerativeAI
# import os
# from dotenv import load_dotenv
# from langchain_core.output_parsers import StrOutputParser
# from langchain_core.runnables import RunnablePassthrough
# from langchain_core.prompts import ChatPromptTemplate

# from langchain.prompts import (
#     ChatPromptTemplate,
#     HumanMessagePromptTemplate,
#     MessagesPlaceholder,
#     SystemMessagePromptTemplate
# )
  
# def main():
#      load_dotenv()
#      st.title("Ask your AI assistant-HR consultant  ğŸ¦œï¸ğŸ”—")
     
#      embeddings = HuggingFaceEmbeddings(model_name="distiluse-base-multilingual-cased-v1",
#                     model_kwargs={'device': 'cpu'})
#      db = FAISS.load_local("vectorstore.faiss",embeddings)
     
#      llm = ChatGoogleGenerativeAI(model='gemini-pro',
#                    google_api_key=os.getenv("GOOGLE_API_KEY"),
#                   temperature=0.6,
#                   convert_system_message_to_human=True)
     
#      general_system_template =  """

#                                     **Ø§Ù†Øª Ø®Ø¨ÙŠØ± Ø§Ø³ØªØ´Ø§Ø±ÙŠ ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¨Ø´Ø±ÙŠØ©**
#                                     **Ø§Ù„Ø³ÙŠØ§Ù‚: **
#                                     {context}
#                                     **Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª: **
#                                     Ø³ÙˆÙ ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø¥Ø³ØªØ´Ø§Ø±ØªÙƒ Ø£Ùˆ Ø³Ø¤Ø§Ù„Ùƒ Ø¹Ù† Ø´ÙŠØ¡Ù‹ Ù…ØªØ¹Ù„Ù‚ ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¨Ø´Ø±ÙŠØ© ÙˆØ§Ù†Øª Ø¨Ø¯ÙˆØ±Ùƒ ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙ‡ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø¥Ø¹Ø·Ø§Ø¡ Ø¬ÙˆØ§Ø¨ ØµØ­ÙŠØ­ ÙˆØ´Ø§Ù…Ù„ ÙˆØ¯Ù‚ÙŠÙ‚ .
#                                     '.Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† ØªØ¹Ø±Ù Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙ‚Ù„ Ù„Ù‡ 'Ø¥Ù†Ùƒ Ù„Ø§ ØªØ¹Ù„Ù… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ø­Ø§ÙˆÙ„ Ø£Ù† ØªØµÙŠØº Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø·Ø±ÙŠÙ‚Ø©Ù Ø£Ø®Ø±Ù‰.

#                                     Ø¥Ø°Ø§ Ù‚Ø§Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø³Ø¤Ø§Ù„Ùƒ Ø¹Ù† Ø´ÙŠØ¡ ØºÙŠØ± Ù…ØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¨Ø´Ø±ÙŠØ© ÙÙ‚Ù„ Ù„Ù‡ Ø¥Ù†Ùƒ
#                                     'ÙŠØ¬Ø¨ Ø£Ù† ØªÙ‚ÙˆÙ… Ø¨Ø³Ø¤Ø§Ù„ÙŠ Ø¹Ù† Ø´ÙŠØ¡ Ù…ØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¨Ø´Ø±ÙŠØ©'.

#                                     """
#      general_user_template = "Question:```{question}```"
     
#      messages = [
#                 SystemMessagePromptTemplate.from_template(general_system_template),
#                 HumanMessagePromptTemplate.from_template(general_user_template)
#     ]
#      qa_prompt = ChatPromptTemplate.from_messages( messages )
     
#      chain= ConversationalRetrievalChain.from_llm(
#                 llm,
#                 retriever=db.as_retriever(),
#                 chain_type="stuff",
#                 verbose=True,
#                 combine_docs_chain_kwargs={'prompt': qa_prompt}
#             ) 
#      def conversational_chat(query):
#             result = chain({"question": query, "chat_history": st.session_state['history']})
#             st.session_state['history'].append((query, result["answer"]))
#             return result["answer"]

#         # Initialize chat history
#      if 'history' not in st.session_state:
#             st.session_state['history'] = []

#         # Initialize messages
#      if 'generated' not in st.session_state:
#             st.session_state['generated'] = ["Hello ! Ask me about " + "HR" + " ğŸ¤—"]

#      if 'past' not in st.session_state:
#             st.session_state['past'] = ["Hey ! ğŸ‘‹"]
            
          

#         # Create containers for chat history and user input
#      response_container = st.container()
#      container = st.container()

#         # User input form
#      with container:
        
#         with st.form(key='my_form', clear_on_submit=True):
#             user_input = st.text_input("Query:", placeholder="Talk with HR AI ğŸ‘‰ (:", key='input')
#             submit_button = st.form_submit_button(label='Send')

#         if submit_button and user_input:
#             with st.spinner('Wait for response...'):
#                 output = conversational_chat(user_input)
#                 st.session_state['past'].append(user_input)
#                 st.session_state['generated'].append(output)

#         # Display chat history
#         if st.session_state['generated']:
#             with response_container:
#                 for i in range(len(st.session_state['generated'])):
#                     message(st.session_state["past"][i], is_user=True, key=str(i) + '_user', avatar_style="big-smile")
#                     message(st.session_state["generated"][i], key=str(i), avatar_style="thumbs")

# if __name__ == '__main__':
#   main() 

import streamlit as st
from streamlit_chat import message
import tempfile
from langchain.document_loaders.csv_loader import CSVLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import CTransformers
from langchain.chains import ConversationalRetrievalChain
from langchain_google_genai import ChatGoogleGenerativeAI
import os
from dotenv import load_dotenv
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate

from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
    SystemMessagePromptTemplate
)
  
def main():
     load_dotenv()
     st.title("Ask your AI assistant-HR consultant  ğŸ¦œï¸ğŸ”—")
     
     embeddings = HuggingFaceEmbeddings(model_name="distiluse-base-multilingual-cased-v1",
                    model_kwargs={'device': 'cpu'})
     db = FAISS.load_local("vectorstore.faiss",embeddings)
     
     llm = ChatGoogleGenerativeAI(model='gemini-pro',
                   google_api_key=os.getenv("GOOGLE_API_KEY"),
                  temperature=0.6,
                  convert_system_message_to_human=True)
     
     general_system_template =  """

                                **Ø§Ù†Øª Ø®Ø¨ÙŠØ± Ø§Ø³ØªØ´Ø§Ø±ÙŠ ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¨Ø´Ø±ÙŠØ©**
                                **Ø§Ù„Ø³ÙŠØ§Ù‚: **
                                
                                    {context}
                                
                                **Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª: **

                                Ø³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„ Ø£Ùˆ Ø·Ù„Ø¨ Ø§Ø³ØªØ´Ø§Ø±Ø© Ø­ÙˆÙ„ Ù…ÙˆØ¶ÙˆØ¹ Ù…ØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¨Ø´Ø±ÙŠØ©.
                                Ø¯ÙˆØ±Ùƒ Ù‡Ùˆ Ù…Ø³Ø§Ø¹Ø¯ØªÙ‡ Ù…Ù† Ø®Ù„Ø§Ù„ ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø© ØµØ­ÙŠØ­Ø© ÙˆØ´Ø§Ù…Ù„Ø© ÙˆØ¯Ù‚ÙŠÙ‚Ø©.
                                Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ØªØ£ÙƒØ¯Ù‹Ø§ Ù…Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ø£Ø®Ø¨Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø°Ù„Ùƒ ÙˆØ­Ø§ÙˆÙ„ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„.
                                
                                **Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø±Ø¯ : **
                                Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŒ ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.
                                Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ù„Ø§ ÙŠØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¨Ø´Ø±ÙŠØ©ØŒ Ø£Ø®Ø¨Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø£Ù† Ø¹Ù„ÙŠÙ‡ Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„ Ù…ØªØ¹Ù„Ù‚ Ø¨Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„.

                                    """
     general_user_template = "Question:```{question}```"
     
     messages = [
                SystemMessagePromptTemplate.from_template(general_system_template),
                HumanMessagePromptTemplate.from_template(general_user_template)
    ]
     qa_prompt = ChatPromptTemplate.from_messages( messages )
     
     chain= ConversationalRetrievalChain.from_llm(
                llm,
                retriever=db.as_retriever(),
                chain_type="stuff",
                verbose=True,
                combine_docs_chain_kwargs={'prompt': qa_prompt}
            ) 
     def conversational_chat(query):
            result = chain({"question": query, "chat_history": st.session_state['history']})
            st.session_state['history'].append((query, result["answer"]))
            return result["answer"]

        # Initialize chat history
     if 'history' not in st.session_state:
            st.session_state['history'] = []

        # Create containers for chat history and user input
     response_container = st.container()
     container = st.container()

        # User input form
     with container:
        
        with st.form(key='my_form', clear_on_submit=True):
            user_input = st.text_input("Query:", placeholder="Talk with HR AI ğŸ‘‰ (:", key='input')
            submit_button = st.form_submit_button(label='Send')

        if submit_button and user_input:
            with st.spinner('Wait for response...'):
                output = conversational_chat(user_input)

        # Display chat history
        if st.session_state['history']:
            with response_container:
                for i in range(len(st.session_state['history'])):
                    message(st.session_state['history'][i][0], is_user=True, key=str(i) + '_user', avatar_style="big-smile")
                    message(st.session_state['history'][i][1], key=str(i), avatar_style="thumbs")

if __name__ == '__main__':
  main() 
